activations
===========

.. only:: html

  Functions
  ---------

  `elu(...) <activations/elu.html>`_
  : Apply exponential exponential linear unit to input.
  `[Clevert et.al, 2015] <https://arxiv.org/abs/1511.07289>`_.

  `exponential(...) <activations/exponential.html>`_
  : Apply exponential activation to input.

  `get(...) <activations/get.html>`_
  : Return the activation callable by identifier.

  `hard_sigmoid(...) <activations/hard_sigmoid.html>`_
  : Apply hard sigmoid function to input.

  `linear(...) <activations/linear.html>`_
  : Apply linear activation to input.

  `relu(...) <activations/relu.html>`_
  : Apply rectified linear unit to input.
  `[Nair & Hinton, 2010] <http://www.csri.utoronto.ca/~hinton/absps/reluICML.pdf>`_.

  `selu(...) <activations/selu.html>`_
  : Apply scaled exponential linear unit to input.
  `[Klambauer et.al, 2017] <https://arxiv.org/abs/1706.02515>`_.

  `sigmoid(...) <activations/sigmoid.html>`_
  : Apply sigmoid function to input.

  `softmax(...) <activations/softmax.html>`_
  : Apply softmax function to input.

  `swish(...) <activations/swish.html>`_
  : Apply swish function to input.
  `[Ramachandran et.al, 2017] <https://arxiv.org/abs/1710.05941>`_.

  `tanh(...) <activations/tanh.html>`_
  : Apply tanh function to input.

.. toctree::
  :hidden:

  activations/elu
  activations/exponential
  activations/get
  activations/hard_sigmoid
  activations/linear
  activations/relu
  activations/selu
  activations/sigmoid
  activations/softmax
  activations/swish
  activations/tanh

.. raw:: html

  <style>
  h1:before {
    content: "Module: tf.keras.";
    color: #103d3e;
  }
  </style>
